# -*- coding: utf-8 -*-
"""Dmart data analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NLGrynInF34eJ3c3fWihyooPJjMvKeEM
"""



# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType, FloatType
from pyspark.sql.functions import col
import logging
from google.colab import files
import os

# Initialize Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    # Step 1: Upload Files Manually
    logger.info("Please upload the files: Product.csv, Sales.csv, and Customer.csv.")
    uploaded = files.upload()
    logger.info("Files uploaded successfully.")

    # Verify files in the current directory
    logger.info("Listing files in current directory:")
    print(os.listdir())

    # Step 2: Initialize PySpark Session
    spark = SparkSession.builder.appName("Sales Data Pipeline").getOrCreate()
    logger.info("PySpark Session Established")

    # Step 3: Define File Paths (Files will be saved in the current directory after upload)
    products_path = "Product.csv"
    sales_path = "Sales.csv"
    customers_path = "Customer.csv"

    # Step 4: Load Data into DataFrames
    logger.info("Loading data into DataFrames...")
    products_df = spark.read.csv(products_path, header=True, inferSchema=True)
    sales_df = spark.read.csv(sales_path, header=True, inferSchema=True)
    customers_df = spark.read.csv(customers_path, header=True, inferSchema=True)
    logger.info("DataFrames loaded successfully")

    # Step 5: Display schemas
    logger.info("Products Schema:")
    products_df.printSchema()

    logger.info("Sales Schema:")
    sales_df.printSchema()

    logger.info("Customers Schema:")
    customers_df.printSchema()

    # Step 6: Data Transformation and Cleaning
    logger.info("Performing data transformations and cleaning...")

    # Rename columns for consistency
    products_df = products_df.withColumnRenamed("Product ID", "product_id")
    customers_df = customers_df.withColumnRenamed("Customer ID", "customer_id")
    sales_df = sales_df.withColumnRenamed("Product ID", "product_id") \
                       .withColumnRenamed("Customer ID", "customer_id")

    # Treat 'Sales' in sales_df as 'price'
    sales_df = sales_df.withColumnRenamed("Sales", "price")

    # Handle missing values
    products_df = products_df.fillna({"Category": "Unknown", "Sub-Category": "Unknown"})
    sales_df = sales_df.dropna(subset=["product_id", "customer_id", "price"])

    # Since 'location' does not exist, handle missing values for 'City' and 'Country'
    customers_df = customers_df.fillna({"age": 0, "City": "Unknown", "Country": "Unknown"})

    # Ensure correct data types
    sales_df = sales_df.withColumn("price", col("price").cast(FloatType()))
    customers_df = customers_df.withColumn("age", col("age").cast(IntegerType()))


    # Step 7: Join DataFrames
    logger.info("Joining DataFrames...")

    # Join sales_df with products_df, and then with customers_df
    sales_products_df = sales_df.join(products_df, on="product_id", how="inner")
    final_df = sales_products_df.join(customers_df, on="customer_id", how="inner")

    # Step 8: Show Final DataFrame
    logger.info("Final Joined DataFrame:")
    final_df.show()

except Exception as e:
    logger.error("An error occurred: %s", str(e))

finally:
    # Stop PySpark Session
    spark.stop()
    logger.info("PySpark session stopped.")

